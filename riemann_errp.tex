
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute)
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\pdfminorversion=4

\documentclass[12pt]{iopart}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{nameref}
\newcommand{\gguide}{{\it Preparing graphics for IOP Publishing journals}}
%Uncomment next line if AMS fonts required
%\usepackage{iopams}  
\begin{document}

\title[]{Stability of spatial covariance in ERPs with high temporal variability}

\author{Ruslan Aydarkhanov$^1$,
Marija U\v{s}\'{c}umli\'{c}$^2$,
Ricardo Chavarriaga$^3$,
Lucian Gheorghe$^4$,
Jos\'e del R Mill\'an$^{5,6,7}$}


\address{$^1$Medical Image Processing Laboratory,
Center for Neuroprosthetics,
Interschool Institute of Bioengineering,
\'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL),
Campus Biotech H4,
1202 Geneva,
Switzerland}
\address{$^2$Nissan International SA,
La Pi\`ece 12,
1180 Rolle,
Switzerland
}
\address{$^3$Zurich University of Applied Sciences, ZHAW,
InIT Institut of Applied Information Technology,
Ob. Kirchgasse 2,
8400 Winterthur,
Switzerland}

\address{$^4$
Advanced Materials and Processing Laboratory, Nissan Research Center, Nissan Motors Co. LTD, 1,
Natsushima, Yokosuka-shi, Kanagawa-ken, 237-8523, Japan
}
\address{$^5$Dept. of Electrical and Computer Engineering,
The University of Texas at Austin,
Austin, TX 78712,
USA}
\address{$^6$Dept. of Neurology,
The University of Texas at Austin,
Austin, TX 78712,
USA}
\address{$^7$\'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL),
Campus Biotech H4,
1202 Geneva,
 Switzerland}
\ead{ruslan.aydarkhanov@epfl.ch}
\vspace{10pt}
%\begin{indented}
%\item[] August 2017
%\end{indented}

\begin{abstract}
\textit{Objective.} Event Related Potentials (ERPs)
reflecting cognitive response to external stimuli,
are widely used in Brain Computer Interfaces (BCI).
ERPs are characterized and typically decoded through a fixed set of
components with particular amplitude and latency. 
However, the classical methods which rely on waveform features
achieve a high decoding performance only with standardized
and well aligned single trials. Since the amplitude and latency
are sensitive to the experimental conditions,
waveform features cannot be successfully applied for challenging tasks or
to generalize across various experimental protocols.
Features based on spatial covariances across channels can potentially overcome
the latency jitter and delays since they aggregate the information
across time.
\textit{Approach.} We compared the performance stability 
of waveform and covariance-based features as well as their combination
in simulated scenarios. We investigate two cases: 1) classifier transfer 
between 3 experiments with Error Related Potentials
and 2) the performance robustness to the added latency jitter.
\textit{Main results.} The features based on spatial covariances
provide a stable performance with a minor decline
under jitter levels of up to $\pm$ 300 ms,
whereas the decoding performance with waveform features
quickly drops from 0.85 to 0.55 AUC. The classifier transfer also
resulted in a significantly more stable performance with covariance-based
features.
\textit{Significance.} Our findings suggest that covariance-based features
can be used to: 1) classify more reliably ERPs
with higher intrinsic variability in more challenging real-life applications
and 2) generalize across related experimental protocols.
\end{abstract}

%
% Uncomment for keywords
\vspace{2pc}
\noindent{\it Keywords\/}: Brain-computer interfaces, Electroencephalography, Event Related Potentials, Covariance, Riemannian geometry
%
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
%
% Uncomment if a separate title page is required
%\maketitle
% 
% For two-column output uncomment the next line and choose [10pt] rather than [12pt] in the \documentclass declaration
%\ioptwocol
%



%There exist various well-established EEG-based BCI paradigms which
%provide a high performance and rely on such phenomena as
%Sensorimotor Rhythms (SMR), Steady-State Evoked Potentials (SSEP),
%or Event-Related Potentials (ERP) and others \cite{hwang_eeg-based_2013}.
%For each paradigm a specific data processing and feature
%extraction steps are required \cite{lotte_review_2018}.

\section{Introduction}
\label{sec:intro}

%Diverse cognitive responses to external stimuli --,
%error detection, object recognition etc. -- reflected as distinguishable modulations of
%the Event-Related Potential (ERP) waveforms, 
Humans are constantly engaged in evaluating sensory percepts 
that are critical to achieve their goals. Such decision
making processes give rise to
brain responses that can be measured as electroencephalography (EEG)
event-related potentials (ERP).
Real-time detection of these ERPs
provide tremendous opportunities for brain-computer interfacing (BCI). 
As an illustration, when
the user observes an error committed by
the system she is interacting with, an Error-Related Potential (ErrP) is elicited
and its detection can be used to correct it \cite{chavarriaga_adaptation_2010}.
Upon each occurrence of such an event, the perception 
and the evaluation processes take variable periods of time.
We investigate the robustness of various ERP properties to these temporal
variations for the purpose of improving their decoding performance for BCI applications.
%The most widely used ERPs in BCI include P300 and ErrP.
%P300 is elicited upon presentation of a rare stimuli in the oddball paradigm
%and it is associated with stimulus categorization whereas
%ErrP is observed when errors are committed by the user, other people or machines.
%These cognitive signals can be applied in various contexts
%and through different modalities: visual, audial, or tactile
%\cite{treder_gaze-independent_2011,schreuder_new_2010,brouwer_tactile_2010,zander_towards_2011,zhang_inferring_2013}.


%Despite the high performance in controlled experimental 
%conditions, the challenge to bring EEG-based BCI to real life
%still remains. 
%In this paper we investigate popular approaches to decode ERPs
%which are often challenged by temporal variability of single trials.

%Brain Computer Interfaces (BCI) provide novel means of interacting
%with machines \cite{wolpaw_braincomputer_2002}. The most efficient technology to acquire
%the neural signal non-invasively is electroencephalography (EEG) \cite{luck_introduction_2005}.

%For BCI application ERPs are classified into one of the two categories 
%based on the EEG signal generated after the event.

%One of the classical approaches to decode ERPs is to 
%use the signal amplitude at specific time points and channels as features
%As opposed to Evoked Potentials triggered by a simple sensory stimulation,
%ERPs are generally associated with higher cognitive processing
%and, therefore, increased latency.for a binary classifier \cite{blankertz_single-trial_2011}.

ERP waveforms consist of multiple components characterized
by a particular latency, polarity and spatial distribution \cite{duncan_event-related_2009}.
The early components are triggered by a sensory stimulation
whereas the late components reflect cognitive processing.
ERPs can be successfully detected in various paradigms under controlled conditions
using preprocessed time signal of the EEG channel activity over the time-window of interest \cite{blankertz_single-trial_2011}.
When facing real world conditions, our cognitive processes may be challenged
by complex and diverse stimuli 
as well as the fact that the environment is dynamic.
As a result, the later ERP components are more prone to latency variability \cite{arico_evaluation_2013},
which compromises the decoding performance.
%Thus, the interaction
%protocol needs to be carefully adjusted and the decoding
%algorithm finely tuned for each scenario \cite{iturrate_task-dependent_2013}.

%The latency is not only greater, it can
%vary across different yet similar tasks and become variable
%at challenging tasks . 
%This inconsistency complicates the decoding of ERP on single trials.

An alternative way to detect ERP is to use features based on the spatial covariance matrices across channels. Such
features emphasize the pair-wise interaction
between channels and the total variance of single channels.
We hypothesize that these features may be a suitable choice
to address the challenge of decoding ERPs with high latency variability 
inherent to real world BCI applications.
Typically, it is advisable to augment the covariance features estimated
on a single trial with the covariances between channels
in single trial and channels in average ERP
of one of the classes. The rationale for this, is that it allows to keep the temporal information 
of ERP dynamics which is considered to be
a valuable source of discriminant information \cite{congedo_new_2013,barachant_plug&play_2014}.
Under ERP latency variability, however, 
augmented covariance matrices may have the same issue as
waveforms.
The impact of latency shifts and jitter have previously been studied
for augmented covariance matrices \cite{barachant_plug&play_2014}
and the performance dropped quickly for delays as short as 60 ms.
An alternative is to consider simple channel-based covariance matrices.

Covariance matrices can be used directly as features \cite{tomioka_classifying_2007}, 
although Euclidean geometry does not properly capture the relationship between them.
It has been shown that treating them under the Riemannian geometry framework,
which will be briefly introduced in the section \nameref{sec:methods},
provides better performance \cite{barachant_classification_2013, zanini_transfer_2018}.

%It has proven to provide performance compared to other frequently used
%approaches in BCI, e.g. waveform features or 
%features extracted with CSP \cite{blankertz_optimizing_2008}.

In order to investigate this hypothesis,
we specifically compare waveform \textit{vs} covariance features,
the benefits of their combination
and the impact of Riemannian \textit{vs} Euclidean geometry under two scenarios.
This analysis is done on the data from error-monitoring experiments \cite{iturrate_latency_2014-1}.
Firstly, we simulate the high temporal
variability by artificially introducing a latency jitter 
on ErrP trials with a low temporal variability.
This corresponds to the expected ERP variability when facing stimuli in
natural environment.
Secondly, we study the robustness of the mentioned features
to the systematic latency shifts between the training and test data. 
Such scenario corresponds to the classifiers transfer between
related ERP protocols. Latency-robust features may be used to reduce
BCI calibration time by direct transfer of the available classifier
to new protocols.


%In this study we explore the robustness of covariance-derived features
%in decoding ERPs to the various perturbations of latency
%and compare them with more traditional temporal waveform features.

%Although one can use the values from covariance matrix
%directly as features for the classifier \cite{tomioka_classifying_2007}, Euclidean geometry is not 
%suitable to describe their relationships \cite{arsigny_geometric_2007}. 
%Since they are 
%Symmetric Positive Definite matrices (SPD),
%they are better characterized in terms of Riemannian geometry.

%It shows promising
%results in generalization across sessions \cite{barachant_classification_2013}
%and transfer learning across subjects \cite{zanini_transfer_2018}.



\section{Materials and Methods}
\label{sec:methods}
\subsection{Data}

The dataset that we chose was specifically recorded to address 
the issue of systematic latency shift in transfer learning \cite{iturrate_latency_2014-1}.
This dataset contains recordings from multiple protocols
on Error-related Potentials (ErrP). It will allow us to test
the transfer learning in real application, in addition to theoretical
investigations and simulations.
In all the protocols participants observed a discrete movement of an object
on the visual display or in physical space.
They evaluated the movement as correct if the object moved towards a marked target or erroneous 
otherwise. The evaluation was made mentally without an overt behavioral response.

In Experiment 1 ($E_1$), a one-dimensional space with 9 positions was shown on a visual display and
a blue square was moving towards a red square (target). In Experiment 3 ($E_3$), participants were seated
in front of a robotic arm, 2 meters away with a transparent panel in between.
The panel contained squared marks in a regular grit forming a two-dimensional space.
The robotic arm moved behind the marks up, down, left or right 
towards one of the corners (targets).
Experiment 2 ($E_2$) was a virtual version of Experiment 3 in which marks and the robotic arm
were rendered on a visual display.
The probability of the erroneous
movement was approximately 30\%. The time between consecutive movements was randomly sampled
from the range [1.7, 4.0] s. 
6 subjects participated in three ErrP experiments.
Further details about the protocols can be found in \cite{iturrate_latency_2014-1}.

The three experiments yielded a similar grand average ERP waveform 
while having systematic differences in latency (Figure \ref{fig:FCz}).
The latency differences were previously estimated to be:
60.42 $\pm$ 25.24, 108.85 $\pm$ 22.86 and 41.02 $\pm$ 12.95 ms
for the $E_1 E_2$, $E_1 E_3$, and $E_2 E_3$ pair of experiments.
A linear classifier
based on waveform features cannot cope with it when trained and applied
on different datasets, the performance drops to random level
for considerable latency shifts. However, in an offline analysis
when the latency shift can be estimated,
e.g. by using a few labelled trials of the new experiment,
a simple correction of latency allows to recover the performance up to the level
obtained within the single protocol dataset \cite{iturrate_latency_2014-1}.


\begin{figure}[!t]
    \centering
    \includegraphics[trim={0cm 0cm 0 0cm},clip,width=0.49\columnwidth]{../images/FCz_all_diff_Topo_compact.pdf}
    \includegraphics[trim={0cm 0cm 0 0cm},clip,width=0.49\columnwidth]{../images/FCz_all_diff_noise300.pdf}
\caption{Grand averages for all three experiments. Left: The waveforms show the difference between
Error and Correct trials at FCz channel. Topographies show the amplitudes for P3 and N4 peaks.
Right: Waveform differences between Error and Correct trials at FCz channel after adding jitter with uniform noise in
the range [-300, 300] ms.}
\label{fig:FCz}
\end{figure}


\subsection{Covariance-based features}

Different sources of neural activity project at the scalp with a specific distribution.
Spatial covariances between EEG channels capture the shape of this distribution
and the strength of the activity averaged across the time-window of interest.
Covariance matrices can be built in different ways.
The simple estimation is done as follows: 
$\mathbf{C} = \frac{1}{s}\mathbf{X}^T\mathbf{X}$ where $\mathbf{X} \in \mathbb{R}^{s \times n}$ is a multichannel
EEG epoch with $n$ channels and $s$ time points.
In case of ERPs, this estimation looses the precise temporal dynamics of the waveform.
In order to compensate for this, a modified version of covariance matrices
was suggested \cite{congedo_new_2013}. 
The epoch $\mathbf{X}$ is augmented with a template $\mathbf{T} \in \mathbb{R}^{s \times k}$
along the channel dimension to build a super epoch
$\mathbf{Z} = [\mathbf{X} \quad \mathbf{T}] \in \mathbb{R}^{s \times (n + k)}$.
The covariance matrix estimated on the augmented signal becomes:
\begin{equation}
    \mathbf{C}_\mathbf{Z} = \frac{1}{s}\mathbf{Z}^T\mathbf{Z} = \frac{1}{s}
    \pmatrix{
        \mathbf{X}^T\mathbf{X} & \mathbf{X}^T\mathbf{T} \cr
        \mathbf{T}^T\mathbf{X} & \mathbf{T}^T\mathbf{T} 
    }
    \label{eq:templ}
\end{equation}

The covariance between $\mathbf{X}$ and $\mathbf{T}$ allows to capture the temporal dynamics
specifically with the relation to the template $\mathbf{T}$. In the context of ERP
experiments, the template is typically an average ERP of one or several classes.
In this case it will reflect how similar a particular epoch is to the average ERP.
Thus the covariance matrix $\mathbf{C}_\mathbf{T}$ contains the combination
of two sources of information: covariances between channels and the similarity to the template waveform.

In high dimensional settings a simple Maximum Likelihood Estimator of covariance matrix is not
stable so a regularization is often applied. In this paper we use
a shrinkage as described in \cite{chen_shrinkage_2010}.

\subsubsection{Riemannian geometry on covariance matrices.}
The natural choice to treat covariance matrices is to vectorize them
and use as a feature vector for further processing and classification.
But most vector-based algorithms assume a Euclidean space, such as PCA \cite{hyvarinen_independent_2001}.
Covariance matrices are always
Symmetric and Positive semi-Definite (SPD), however,
Euclidean geometry is not well suited for SPD matrices due to various
drawbacks \cite{arsigny_geometric_2007}, so they can be characterized 
better with Riemannian geometry while improving the performance.
%For example, a popular approach relying on covariance matrices for feature extraction,
%namely, Common Spatial Patterns (CSP) can be improved within Riemannian framework \cite{barachant_common_2010}.

Riemannian geometry is the branch of mathematics that studies
smooth spaces locally behaving like a Euclidean space.
The main feature of Riemannian space consists in the way the distances are defined.
While Euclidean space has a constant distance metric in all points,
the Riemannian metric smoothly changes along the space.
As a simple visual example, imagine a curvy surface of a sphere.
Although the surface is a 2-dimensional space,
we cannot directly compute the distance between 2 remote points.
However, around each point distances can be locally approximated
with $\mathbb{R}^2$ Euclidean space.

One of the simple ways to introduce the benefits from Riemannian geometry
into classification of covariance matrices is project them
on a tangent space \cite{barachant_multiclass_2012}. 
The tangent space is a Euclidean space which allows
to leverage the standard vector-based algorithms.
The approach requires two steps:
\begin{enumerate}
    \item To find a geometric mean $\bar{\mathbf{C}}$ of the sample of covariance matrices:
        \begin{equation}
            \bar{\mathbf{C}} = \textrm{argmin}_\mathbf{C} \sum_{i=1}^N \delta^2 (\mathbf{C}_i, \mathbf{C})
            \label{eq:geommean}
        \end{equation}
        where $\mathbf{C}_i$ denotes a covariance matrix, and $\delta$ is a 
        Riemannian distance between covariance matrices.
    \item The relationship between covariance matrices can be approximated with
        Euclidean geometry around the geometric mean $\bar{\mathbf{C}}$ by projecting them on a tangent
        space $\mathbf{S}_{\bar{\mathbf{C}}}$ which is called logarithmic mapping $\mathrm{Logm}(\cdot)$:
        \begin{equation}
            \mathbf{S}_{\bar{\mathbf{C}}} = \mathrm{Logm}( \mathbf{C}) = \bar{\mathbf{C}}^{1/2}
            \textrm{logm}(\bar{\mathbf{C}}^{-1/2}\mathbf{C}\bar{\mathbf{C}}^{-1/2})\bar{\mathbf{C}}^{1/2}
            \label{eq:logm}
        \end{equation}
        where $\textrm{logm}$ denotes the logarithm of a matrix \cite{berger_panoramic_2003}.
\end{enumerate}

\subsection{Data processing and feature extraction}
All the EEG was filtered with a Butterworth band-pass filter of order 4 within the band [1, 10] Hz
forward and backward and downsampled from 512 Hz to 128 Hz. Then the signal was spatially
filtered with common-average-reference (CAR). Further feature extraction was done
in the time window of [200, 1000] ms after the event (correct or erroneous movement).
We explore and compare different sets
of features, which include ERP waveform and covariance matrices.

For the classification we compute the following features (Figure \ref{fig:Features}):
\begin{enumerate}[label=F\arabic*]
    \item \textbf{Waveform}. Waveform of the ERP epoch $\mathbf{X}$, i.e. the amplitude at all channels and time points. Due to high
        dimensionality (1648) we applied PCA and keep components with highest eigenvalue
        which explain 90\% of the variance. It resulted into 60 +/- 8 features.
    \item \textbf{Euclid}. Covariance matrices $\mathbf{C}$ computed with shrinkage to improve stability, yielding 136 features.
    \item \textbf{Riemann}. Projections of shrinkaged covariance matrices on a tangent space $\mathbf{S}$. The projection reference
        point was the geometric mean estimated from the training data.
        The projection does not affect the dimensionality, yielding 136 features (see the equation\ref{eq:logm}).
    \item \textbf{Riemann+}. Shrinkaged covariance matrices $\mathbf{C_Z}$ estimated on super epochs augmented with the averaged ERPs
        of each class $\mathbf{Z}$. ERPs were estimated from the training data.
        Such an augmentation produced
        a high number of features (1176), so we decided to preselect 8 channels which
        drastically reduced the number of features to 300. We automatically chose
        the channels according to the mean Fisher score across all time points \cite{duda_pattern_2001}.
        The covariance matrices $\mathbf{C_Z}$ were projected on a corresponding tangent space $\mathbf{S_Z}$ .
\end{enumerate}


\begin{figure}[!t]
    \centering
    \includegraphics[trim={0cm 0cm 0 0cm},clip,width=0.5\columnwidth]{../images/Features2.pdf}
\caption{Steps of feature construction for each EEG epoch. The following operations were used:
    Vect - vectorization of unique values (only upper triangle for SPD matrices), Cov - estimation of covariance matrix with shrinkage,
Select Channels - selection of 8 most discriminant channels,
Augment - augmentation of the epoch with grand average of erroneous and correct classes,
Logm - logarithmic mapping on the tangent space.}
\label{fig:Features}
\end{figure}

\subsection{Classification performance evaluation with respect to the latency}

To classify erroneous vs correct trials we applied penalized logistic regression (PLR), robust to overfiting and
outliers \cite{parra_recipes_2005}. The regularization parameter was chosen by 4-fold cross-validation in the
training dataset. The data was split to preserve the
temporal relationships (non-randomized trials).

Additionally, we standardized all features by z-score with mean and standard deviation
obtained from the training data because we used a regularized classifier \cite{hastie_elements_2009}.
Since the classes
were not balanced in ErrP protocols, we measured the performance by area under the ROC curve (AUC) 
\cite{bradley_use_1997}.


\subsubsection*{Baseline performance.} 
As a baseline we estimated the performance of all features in a leave-one-run-out cross validation.
Together with the hyperparameter optimization the overall procedure was structured as
a nested cross validation.
%\cite{iturrate_latency_2014-1}


\subsubsection*{Latency jitter.} 
The authors of the used dataset reported high decoding performance of AUC $> 80$ with a linear classifier
when trained and applied within the same protocol.
This suggests that ERP waveforms are consistent across single trials.
In order to investigate the robustness of different features
to the temporal variability we introduced a random jitter to the time window
for all data which included both training and test epochs. The amount of jitter was sampled from a uniform distribution.
Three gradual jitter levels were [-50, 50], [-100, 100], [-200, 200] and [-300, 300] ms.
This procedure was repeated on all three experiments.

\subsubsection*{Classifier transfer.}
The three experiments proved to evoke similar ErrP waveforms on grand averages with a 
systematic shift in latency. Without any fine-tuning and additional
transformation we transfer classifiers within the same subject
between experiments. Classifier trained on all data of one experiment
is tested on all data of the other two, providing a single AUC value per pair.
This procedure is repeated for all pairs of the experiments and all feature sets.


\section{Results}
\label{sec:results}

\subsection{Baseline performance}

\begin{figure}[!t]
    \centering
    \includegraphics[trim={0cm 1.5cm 0 2.2cm},clip,width=0.8\columnwidth]{../images/RiemannERP_Jitter_Summary.png}
\caption{Robustness of the features to simulated temporal jitter.
    Each figure represents one of the experiments. The data shows
    AUC as mean and standard deviation across all subjects.}
\label{fig:jitter}
\end{figure}

First, we can see that all features perform well on the
original datasets without jitter (Figure \ref{fig:jitter}). The average AUC across subjects is between 75 and 90.
There is a statistically significant difference across feature sets obtained with two-way repeated
measures ANOVA, the factors being the feature and the experiments (p-value $< 0.001$
(0.000186)). By breaking down the difference into pair-wise comparison with post-hoc
two-way repeated measures ANOVA, we can see that the difference is mainly driven by
``Riemann+'' features (p-value $< 0.01$ for all pair-wise comparisons with other
features). So the combination of discriminant information by means
of covariance matrices on super epochs gives the best results. Processing of
covariance matrices with Riemannian geometry significantly improves the performance
as opposed to Euclidean geometry (p-value $= 0.03$).


\subsection{Robustness to latency jitter}

We introduced different levels of latency jitter to compare the temporal robustness
of different feature sets.
The performance change for different levels of jitter is shown on Figure \ref{fig:jitter}.
The overall trend is a reduction in performance with the increase in temporal jitter for all features.
The significance of the effect is tested on each experiment independently
with 2-way repeated measures ANOVA for the jitter level and the feature
as factors. Both main effects are significant (p-values $<0.001$). 
Regardless the experiment, the strongest effect is obtained for the interaction
of factors (p-values $< 0.0001$) for all the experiments.

The performance decreases more
rapidly with jitter for Waveform features, which
drops to nearly chance level for jitter level above 200 ms.
The post-hoc comparison of ``Euclid'' or ``Riemann'' against ``Riemann+'' features gives non-significant
difference on the features, however, the interaction is significant (p-values $< 0.0001$).
It allows us to conclude that simple covariance matrices are most stable against the jitter
regardless of whether they are treated with Euclidean or Riemannian geometry.

\subsection{Classifier transfer between protocol}
We trained separate classifiers on the datasets of each protocol
and assessed their performance on the two remaining 
protocols (Figure \ref{fig:transfer}).
As it was the case for the latency variations, 
waveform features demonstrate the biggest drop
in performance. The transfer from $E_1$ to either $E_2$ or $E_3$
leads to a random classification performance. The respective grand average 
latency differences are 60 and 100 ms.

In contrast to the within-dataset performance, the ``Riemann+'' approach which
combines both types of information
does not provide best results when transferring between protocols.
Interestingly, covariance matrices are more robust
especially when the latency shift is bigger.
There is a statistically significant difference across features
for all training/test pairs, with p-values $< 0.0001$ (one-way ANOVA).
Furthermore, there is no difference in performance between simple covariance matrices
and the ones transformed with Riemannian geometry.


\begin{figure*}[!t]
    \centerline{\includegraphics[trim={2.5cm 0cm 3cm 0.5cm},clip,width=\columnwidth]{../images/RiemannERP_Transfer_Matrix.png}}
    \caption{Cross-protocol classifier transfer. First three matrices show AUC estimated
    on test dataset (y-axis) with a classifier trained on a training dataset (x-axis).
    The right-most matrix provides p-values in $\mathrm{log_{10}}(\cdot)$ obtained with ANOVA when comparing features
    for each train/test pair.}
\label{fig:transfer}
\end{figure*}

\section{Discussion and Conclusion}
\label{sec:discussion}

Various studies have shown high 
performance of ERP-based BCIs in controlled experimental conditions.
%when the classifiers are trained individually for each subject.
Stimuli diversity or any other change in experimental conditions, however,
can affect ERP waveforms and degrade classification performance.
%So it is not always feasible to train a single classifier 
%even within a single paradigm, like ErrP or P300.
On one side, as we have seen through the set of experiments on error monitoring,
a small modification such as a transition from 1D to 2D scenario
or virtual to physical can influence the ERP latency.
On the other side, more complex stimuli
could lead to a higher temporal variability of ERP within the same protocol  \cite{arico_evaluation_2013}.

%In order to ensure a high performance of a system the algorithm
%is trained individually for each subject and each experiment.
%The stimuli must be clear and simple to recognize
%in order to evoke a standardized EEG waveform. And the classifier
%must be fine-tuned for the given ERP waveform.

In this study we investigated the robustness of different types of features
to the variability and systematic shifts in latency using real ErrP data.
We conducted two analyses: 1) simulated ErrP temporal variability (latency jitter)
and 2) classifier transfer between protocols where an ErrP latency shift is observed.
%For our experiments we selected a dataset obtained on three
%different ErrP protocols. The ERPs within each protocol are
%well aligned but have latency differences between them.

We showed that both types of features -- spatial covariances and the ERP waveform --
carry sufficient information for high decoding performance (AUC $> 75$) for all three experiments.
Waveform features outperform covariance features, yet their combination
by the means of the augmenting covariance matrices results
in the highest performance. This can be explained by complementary 
information contained in the spatio-temporal relations between channels.
%contain more discriminant power on the original dataset
%since they produce higher classification performance in all three experiments.
%But the method which combines both channel covariances and the waveform
%provides the highest performance.
When facing ERP latency shifts or jitter, however,
spatial covariance features are more robust in contrast to waveform features.
Our results indicate that classification based on waveform features approach a chance level
for jitter levels above 200 ms, or latency differences
between experiments that exceed 100 ms.
%In both scenarios with classifier transfer and latency jitter
The features combining both information sources do not provide 
the best performance, it is intermediate
between the performances of covariance and waveform feature sets alone.
This is expected as it is constrained by limitation
of the waveform features. 

%There is no single recipe that can fit all application cases.
%It is not always feasible to assess the temporal variability
%of ERPs, so a new experimental scenarios both approaches
%could be evaluated.
%However, experiments with ambiguous and complex stimuli which require more time
%for evaluation could benefit from temporally robust covariance features.
%This includes distinguishing between similar stimuli, dynamic
%visual stimuli or additional tasks.
%Better generalization capabilities can also allow a more efficient and quick
%calibration procedure in case of the transfer between related ERP-based protocols.

We recognize a potential of the covariance-based features
for real world BCI applications where ERP variability is inevitable. 
One cannot control all the aspects of the natural environment --
diverse stimuli may require different levels of cognitive
effort reflected in the ERP latency \cite{kutas_augmenting_1977}.
%Latency is mainly affected by the stimulus evaluation time \cite{kutas_augmenting_1977}.
Moreover, there is a requirement on efficient calibration and 
a quick switch to a new setup, which could be fulfilled with the approach presented here. 

\ack

The authors thank Nissan Motor Co. Ltd for supporting
this work.

\section*{References}

\bibliographystyle{unsrt}
%\bibliographystyle{iopart-num}
\bibliography{RiemannERP_paper}

\end{document}

